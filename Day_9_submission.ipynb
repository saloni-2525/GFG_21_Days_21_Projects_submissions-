{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ba4332e"
      },
      "source": [
        "# Transfer Learning with Pre-trained Models on CIFAR-100\n",
        "\n",
        "## Introduction\n",
        "This notebook explores the application of transfer learning using popular pre-trained convolutional neural network architectures to address the image classification task on the CIFAR-100 dataset. Leveraging models pre-trained on the large-scale ImageNet dataset, such as ResNet50, VGG16, and MobileNetV2, allows us to benefit from their learned feature extraction capabilities. The goal is to adapt these powerful models to the finer-grained classification challenges presented by CIFAR-100, which consists of 100 distinct classes. This approach significantly reduces the need for training deep models from scratch on a relatively smaller dataset, often leading to improved performance and faster convergence.\n",
        "\n",
        "## Project Flow\n",
        "\n",
        "1.  **Data Loading and Preprocessing**: Load the CIFAR-100 dataset and apply the necessary preprocessing steps tailored for each pre-trained model (ResNet50, VGG16, MobileNetV2). This involves scaling pixel values and potentially resizing images to match the input requirements of the chosen architectures.\n",
        "\n",
        "2.  **Model Preparation**:\n",
        "    *   Load pre-trained models (ResNet50, VGG16, MobileNetV2) without their top classification layers.\n",
        "    *   Add new custom classification layers suitable for the 100 classes of CIFAR-100.\n",
        "    *   Freeze the layers of the pre-trained base models to retain the learned features during initial training.\n",
        "    *   Compile the models with an appropriate optimizer, loss function, and metrics.\n",
        "\n",
        "3.  **Fine-Tuning and Training**:\n",
        "    *   Optionally unfreeze a portion of the top layers of the pre-trained models to allow for fine-tuning on the CIFAR-100 data.\n",
        "    *   Train the modified models on the preprocessed training data, monitoring performance on the validation set.\n",
        "\n",
        "4.  **Model Evaluation**: Evaluate the performance of each trained model (ResNet50, VGG16, MobileNetV2) on the held-out test dataset using relevant metrics such as accuracy.\n",
        "\n",
        "5. **Comparison of Results**: Compare the performance of the different models to understand the effectiveness of each architecture for transfer learning on CIFAR-100."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsZFaJzx8xtH"
      },
      "source": [
        "## 1. Data Loading and Preprocessing\n",
        "Load the CIFAR-100 dataset and prepare it for transfer learning by applying appropriate preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-wi1GdT8xtH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar100\n",
        "\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input as preprocess_resnet50\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input as preprocess_vgg16\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as preprocess_mobilenetv2\n",
        "\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = cifar100.load_data()\n",
        "\n",
        "X_train_resnet50 = preprocess_resnet50(X_train)\n",
        "X_test_resnet50  = preprocess_resnet50(X_test)\n",
        "\n",
        "X_train_vgg16 = preprocess_vgg16(X_train)\n",
        "X_test_vgg16  = preprocess_vgg16(X_test)\n",
        "\n",
        "X_train_mobilenetv2 = preprocess_mobilenetv2(X_train)\n",
        "X_test_mobilenetv2  = preprocess_mobilenetv2(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaz94rVB8xtI"
      },
      "source": [
        "## 2. Model Preparation\n",
        "Load and modify pre-trained models to fit the CIFAR-100 classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBs9PdAu8xtI"
      },
      "source": [
        "### 2.1 Using ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy0srMxN8xtI"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "\n",
        "# Load pre-trained ResNet50 model without the top layer\n",
        "base_model_resnet50 = ResNet50(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "\n",
        "x = GlobalAveragePooling2D()(base_model_resnet50.output)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(100, activation='softmax')(x) # output layer\n",
        "\n",
        "model_resnet50 = Model(inputs=base_model_resnet50.input, outputs=predictions)\n",
        "\n",
        "model_resnet50.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG3ZpE618xtI"
      },
      "source": [
        "### 2.2 Using VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoOfgJMD8xtI"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "\n",
        "# Load pre-trained VGG16 model without the top layer\n",
        "base_model_vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "# Add new layers\n",
        "x = GlobalAveragePooling2D()(base_model_vgg16.output)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "predictions = Dense(100, activation='softmax')(x)\n",
        "model_vgg16 = Model(inputs=base_model_vgg16.input, outputs=predictions)\n",
        "\n",
        "model_vgg16.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPvKjypT8xtJ"
      },
      "source": [
        "### 2.3 Using MobileNetV2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVE5QT-r8xtJ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "\n",
        "# Load pre-trained MobileNetV2 model without the top layer\n",
        "base_model_mobilenetv2 = MobileNetV2(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "# Add new layers\n",
        "x = GlobalAveragePooling2D()(base_model_mobilenetv2.output)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "predictions = Dense(100, activation='softmax')(x)\n",
        "model_mobilenetv2 = Model(inputs=base_model_mobilenetv2.input, outputs=predictions)\n",
        "\n",
        "model_mobilenetv2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4LO5ynU8xtJ"
      },
      "source": [
        "## 3. Fine-Tuning and Training\n",
        "Unfreeze some of the top layers of the pre-trained models and continue training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5ElWybb8xtJ"
      },
      "outputs": [],
      "source": [
        "epochs = 3\n",
        "\n",
        "print(len(model_resnet50.layers))\n",
        "print(len(model_vgg16.layers))\n",
        "print(len(model_mobilenetv2.layers))\n",
        "\n",
        "\n",
        "# Fine-tuning ResNet50\n",
        "for layer in model_resnet50.layers[:-30]:\n",
        "    layer.trainable = False\n",
        "\n",
        "for layer in model_resnet50.layers[-30:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "history_resnet50 = model_resnet50.fit(X_train_resnet50, y_train, epochs=epochs, validation_data=(X_test_resnet50, y_test))\n",
        "\n",
        "\n",
        "\n",
        "# Fine-tuning VGG16\n",
        "for layer in model_vgg16.layers[:-5]:\n",
        "    layer.trainable = False\n",
        "\n",
        "for layer in model_vgg16.layers[-5:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "history_vgg16 = model_vgg16.fit(X_train_vgg16, y_train, epochs=epochs, validation_data=(X_test_vgg16, y_test))\n",
        "\n",
        "\n",
        "\n",
        "# Fine-tuning MobileNetV2\n",
        "for layer in model_mobilenetv2.layers[:-40]:\n",
        "    layer.trainable = False\n",
        "\n",
        "for layer in model_mobilenetv2.layers[-40:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "history_mobilenetv2 = model_mobilenetv2.fit(X_train_mobilenetv2, y_train, epochs=epochs, validation_data=(X_test_mobilenetv2, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAJbqP6T8xtJ"
      },
      "source": [
        "## 4. Model Evaluation\n",
        "Evaluate each model on the test dataset to compare their performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyEKFHnS8xtJ"
      },
      "outputs": [],
      "source": [
        "acc_resnet50    = model_resnet50.evaluate(X_test_resnet50, y_test)[1]\n",
        "acc_vgg16       = model_vgg16.evaluate(X_test_vgg16, y_test)[1]\n",
        "acc_mobilenetv2 = model_mobilenetv2.evaluate(X_test_mobilenetv2, y_test)[1]\n",
        "\n",
        "print(f'ResNet50 Accuracy: {acc_resnet50:.2f}')\n",
        "print(f'VGG16 Accuracy: {acc_vgg16:.2f}')\n",
        "print(f'MobileNetV2 Accuracy: {acc_mobilenetv2:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_history(history, model_name):\n",
        "    \"\"\"Plots training and validation accuracy and loss.\"\"\"\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title(f'{model_name} - Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title(f'{model_name} - Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot history for each model\n",
        "plot_history(history_resnet50, 'ResNet50')\n",
        "plot_history(history_vgg16, 'VGG16')\n",
        "plot_history(history_mobilenetv2, 'MobileNetV2')"
      ],
      "metadata": {
        "id": "vyGIv42GqOVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b8188a6"
      },
      "source": [
        "# Save the models\n",
        "model_resnet50.save('resnet50_cifar100.h5')\n",
        "model_vgg16.save('vgg16_cifar100.h5')\n",
        "model_mobilenetv2.save('mobilenetv2_cifar100.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b325d892"
      },
      "source": [
        "## Project Summary\n",
        "\n",
        "*   **Data Loading and Preprocessing**: Loaded the CIFAR-100 dataset and preprocessed images using model-specific functions (ResNet50, VGG16, MobileNetV2).\n",
        "*   **Model Adaptation**: Loaded pre-trained ResNet50, VGG16, and MobileNetV2 models (without top layers), added new classification layers for 100 classes, and initially froze base model layers.\n",
        "*   **Model Compilation**: Compiled each modified model with the 'adam' optimizer, 'sparse\\_categorical\\_crossentropy' loss, and 'accuracy' metric.\n",
        "*   **Fine-Tuning (Example)**: Demonstrated fine-tuning by unfreezing top layers of the ResNet50 model and training it for 10 epochs.\n",
        "*   **Model Evaluation**: Evaluated the trained models on the test set to determine and compare their classification accuracies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91c09f1b"
      },
      "source": [
        "## Project Assignment: Transfer Learning on Oxford Flowers 102 Dataset Documentation\n",
        "\n",
        "This document outlines the steps for the project assignment on applying transfer learning to the Oxford Flowers 102 dataset.\n",
        "\n",
        "**Objective:** Apply transfer learning techniques using pre-trained convolutional neural networks (ResNet50, VGG16, and MobileNetV2) to classify images from the Oxford Flowers 102 dataset. Compare the performance of the different models on this dataset.\n",
        "\n",
        "**Dataset:** Oxford Flowers 102 - A dataset of 102 categories of flowers. You will load this dataset using TensorFlow Datasets.\n",
        "\n",
        "**Assignment Steps:**\n",
        "\n",
        "1.  **Introduce the Assignment:**\n",
        "    *   Create a markdown cell to introduce the assignment.\n",
        "    *   Explain the goal: to apply transfer learning for flower classification using the Oxford Flowers 102 dataset.\n",
        "    *   Mention the pre-trained models to be used: ResNet50, VGG16, and MobileNetV2.\n",
        "    *   Briefly describe the Oxford Flowers 102 dataset.\n",
        "\n",
        "2.  **Data Loading and Exploration:**\n",
        "    *   Generate a code cell to load the 'oxford_flowers102:2.1.1' dataset using `tfds.load()`. (Check available versions if needed)\n",
        "    *   Split the dataset into training, validation, and testing sets (this dataset has these splits).\n",
        "    *   Explore the dataset to understand its structure, the number of classes (102), and the image dimensions. You can display some sample images and their labels.\n",
        "\n",
        "3.  **Data Preprocessing:**\n",
        "    *   Generate a code cell for preprocessing the images from the Oxford Flowers 102 dataset.\n",
        "    *   This will involve resizing the images to the input size required by the pre-trained models (e.g., 224x224 for VGG16 and ResNet50, MobileNetV2 might have different requirements, so check the documentation).\n",
        "    *   Apply the model-specific preprocessing functions (e.g., `tf.keras.applications.resnet50.preprocess_input`) to normalize the pixel values.\n",
        "    *   Apply one-hot encoding to the labels.\n",
        "    *   Batch and prefetch the datasets for efficient training.\n",
        "\n",
        "4.  **Model Adaptation and Training:**\n",
        "    *   For each of the three models (ResNet50, VGG16, MobileNetV2):\n",
        "        *   Generate a code cell to load the pre-trained model from `tf.keras.applications`, excluding the top classification layer and specifying the correct input shape for the preprocessed images.\n",
        "        *   Add new custom layers on top of the base model for classifying 102 classes. This typically involves a GlobalAveragePooling2D layer and a Dense layer with 102 units and a 'softmax' activation.\n",
        "        *   Freeze the layers of the pre-trained base model.\n",
        "        *   Compile the model with an appropriate optimizer (e.g., 'adam'), loss function ('categorical\\_crossentropy' since you'll use one-hot encoded labels), and metrics (e.g., 'accuracy').\n",
        "        *   Generate a code cell to train the compiled model on the preprocessed training data for a suitable number of epochs. Use the validation data to monitor performance during training. Consider using callbacks like ModelCheckpoint and EarlyStopping.\n",
        "        *   Additionally, train the model on the validation split as well, as this dataset provides a separate validation set.\n",
        "        *   Optionally, unfreeze some of the top layers of the base model and fine-tune the model with a lower learning rate.\n",
        "\n",
        "5.  **Model Evaluation:**\n",
        "    *   Generate a code cell to evaluate each trained model on the preprocessed test dataset.\n",
        "    *   Print the loss and accuracy for each model.\n",
        "\n",
        "6.  **Assignment Questions/Tasks:**\n",
        "    *   Add markdown cells with questions for students to answer:\n",
        "        *   Which model performed best on the Oxford Flowers 102 dataset and why do you think that is the case?\n",
        "        *   Compare the performance of the models on Oxford Flowers 102 to their performance on CIFAR-100 (from the original notebook). What differences do you observe and why?\n",
        "        *   Discuss the effect of transfer learning on this dataset.\n",
        "        *   Explain the steps you took for data preprocessing and why they were necessary.\n",
        "        *   Describe the model architectures you used and how you adapted them for the Oxford Flowers 102 dataset.\n",
        "        *   What challenges did you encounter during this assignment and how did you address them?\n",
        "    *   Suggest optional tasks, such as:\n",
        "        *   Experiment with different hyperparameters (learning rate, number of epochs, batch size).\n",
        "        *   Implement data augmentation techniques.\n",
        "        *   Try fine-tuning different numbers of layers.\n",
        "        *   Visualize sample predictions and analyze misclassifications.\n",
        "\n",
        "7.  **Conclusion/Submission:**\n",
        "    *   Add a markdown cell for students to write a brief conclusion summarizing their findings.\n",
        "    *   Provide instructions on how they should submit their completed notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INTRODUCTION OF ASSIGNMENT**\n",
        "\n",
        "In this assignment, the task is to apply transfer learning for image classification on the Oxford Flowers 102 dataset. The objective is to build flower classifiers by adapting powerful pre-trained convolutional neural networks instead of training deep models from scratch.​\n",
        "\n",
        "The models to be used are ResNet50, VGG16, and MobileNetV2, all originally trained on the large-scale ImageNet dataset. These networks will serve as feature extractors, on top of which new classification layers will be added to recognize 102 flower categories.​\n",
        "\n",
        "The Oxford Flowers 102 dataset contains images from 102 different flower categories that commonly occur in the United Kingdom, with each class having between 40 and 258 images and exhibiting large variations in scale, pose, and lighting. This makes it a realistic and challenging benchmark for evaluating the effectiveness of transfer learning approaches to fine-grained visual classification."
      ],
      "metadata": {
        "id": "i3EXkVFwKJj4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80567ff3"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Load the Oxford Flowers 102 dataset\n",
        "try:\n",
        "    dataset, info = tfds.load('oxford_flowers102:2.1.1', with_info=True, as_supervised=True)\n",
        "\n",
        "    # Split the dataset into training, validation, and testing sets\n",
        "    train_dataset = dataset['train']\n",
        "    validation_dataset = dataset['validation']\n",
        "    test_dataset = dataset['test']\n",
        "# Print key info\n",
        "    print(\"Number of classes:\", info.features['label'].num_classes)\n",
        "    print(\"Image shape:\", info.features['image'].shape)\n",
        "    print(\"Train examples:\", info.splits['train'].num_examples)\n",
        "    print(\"Validation examples:\", info.splits['validation'].num_examples)\n",
        "    print(\"Test examples:\", info.splits['test'].num_examples)\n",
        "\n",
        "    # Class names list\n",
        "    class_names = info.features['label'].names\n",
        "\n",
        "    # Show some sample images\n",
        "    def show_samples(ds, n=9):\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        for i, (img, label) in enumerate(ds.take(n)):\n",
        "            ax = plt.subplot(3, 3, i + 1)\n",
        "            plt.imshow(img)\n",
        "            plt.title(f\"{label.numpy()} - {class_names[label.numpy()]}\")\n",
        "            plt.axis(\"off\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    show_samples(train_dataset)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Data Preprocessing\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input as preprocess_resnet50\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input as preprocess_vgg16\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as preprocess_mobilenetv2\n",
        "\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# num_classes from ds_info in Task 2\n",
        "num_classes = info.features[\"label\"].num_classes\n",
        "\n",
        "def make_pipeline(dataset, preprocess_fn):\n",
        "    \"\"\"Resize, apply model-specific preprocessing, one-hot encode labels, batch & prefetch.\"\"\"\n",
        "    def _process(image, label):\n",
        "        # Resize to 224x224 (works for ResNet50, VGG16, MobileNetV2) [web:12][web:9]\n",
        "        image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "        # Model-specific normalization\n",
        "        image = preprocess_fn(image)\n",
        "        # One-hot encode labels (102 classes)\n",
        "        label = tf.one_hot(label, num_classes)\n",
        "        return image, label\n",
        "\n",
        "    return (dataset\n",
        "            .map(_process, num_parallel_calls=AUTOTUNE)\n",
        "            .shuffle(1000)\n",
        "            .batch(BATCH_SIZE)\n",
        "            .prefetch(AUTOTUNE))\n",
        "\n",
        "# Create separate pipelines for each backbone\n",
        "train_resnet = make_pipeline(train_dataset, preprocess_resnet50)\n",
        "val_resnet   = make_pipeline(validation_dataset, preprocess_resnet50)\n",
        "test_resnet  = make_pipeline(test_dataset, preprocess_resnet50)\n",
        "\n",
        "train_vgg = make_pipeline(train_dataset, preprocess_vgg16)\n",
        "val_vgg   = make_pipeline(validation_dataset, preprocess_vgg16)\n",
        "test_vgg  = make_pipeline(test_dataset, preprocess_vgg16)\n",
        "\n",
        "train_mnet = make_pipeline(train_dataset, preprocess_mobilenetv2)\n",
        "val_mnet   = make_pipeline(validation_dataset, preprocess_mobilenetv2)\n",
        "test_mnet  = make_pipeline(test_dataset, preprocess_mobilenetv2)\n"
      ],
      "metadata": {
        "id": "8_h9LQ2tLty-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Simplified Model Adaptation and Training\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50, VGG16, MobileNetV2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "IMG_SIZE = 224\n",
        "INPUT_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
        "num_classes = info.features[\"label\"].num_classes  # from Task 2\n",
        "\n",
        "def add_classification_head(base_model, dense_units=512, dropout_rate=0.5):\n",
        "    x = GlobalAveragePooling2D()(base_model.output)\n",
        "    x = Dense(dense_units, activation=\"relu\")(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
        "    return Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "# Common callbacks\n",
        "def make_callbacks():\n",
        "    return [\n",
        "        EarlyStopping(monitor=\"val_accuracy\", patience=5, restore_best_weights=True),\n",
        "        ReduceLROnPlateau(monitor=\"val_loss\", factor=0.3, patience=3, verbose=1),\n",
        "    ]\n",
        "\n",
        "EPOCHS = 20\n",
        "\n",
        "# -------- ResNet50 --------\n",
        "base_resnet = ResNet50(weights=\"imagenet\", include_top=False, input_shape=INPUT_SHAPE)\n",
        "base_resnet.trainable = False\n",
        "model_resnet = add_classification_head(base_resnet, dense_units=512)\n",
        "model_resnet.compile(optimizer=\"adam\",\n",
        "                     loss=\"categorical_crossentropy\",\n",
        "                     metrics=[\"accuracy\"])\n",
        "\n",
        "history_resnet = model_resnet.fit(\n",
        "    train_resnet,\n",
        "    validation_data=val_resnet,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=make_callbacks()\n",
        ")\n",
        "\n",
        "# Optional fine-tuning (unfreeze top layers of base_resnet)\n",
        "for layer in base_resnet.layers[-30:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "model_resnet.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history_resnet_ft = model_resnet.fit(\n",
        "    train_resnet,\n",
        "    validation_data=val_resnet,\n",
        "    epochs=10,\n",
        "    callbacks=make_callbacks()\n",
        ")\n",
        "\n",
        "# -------- VGG16 --------\n",
        "base_vgg = VGG16(weights=\"imagenet\", include_top=False, input_shape=INPUT_SHAPE)\n",
        "base_vgg.trainable = False\n",
        "model_vgg = add_classification_head(base_vgg, dense_units=512)\n",
        "model_vgg.compile(optimizer=\"adam\",\n",
        "                  loss=\"categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "\n",
        "history_vgg = model_vgg.fit(\n",
        "    train_vgg,\n",
        "    validation_data=val_vgg,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=make_callbacks()\n",
        ")\n",
        "\n",
        "# -------- MobileNetV2 --------\n",
        "base_mnet = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=INPUT_SHAPE)\n",
        "base_mnet.trainable = False\n",
        "model_mnet = add_classification_head(base_mnet, dense_units=256)\n",
        "model_mnet.compile(optimizer=\"adam\",\n",
        "                   loss=\"categorical_crossentropy\",\n",
        "                   metrics=[\"accuracy\"])\n",
        "\n",
        "history_mnet = model_mnet.fit(\n",
        "    train_mnet,\n",
        "    validation_data=val_mnet,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=make_callbacks()\n",
        ")\n"
      ],
      "metadata": {
        "id": "ihrGW5QCL4e5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: Model Evaluation on Test Set\n",
        "\n",
        "# Evaluate ResNet50\n",
        "test_loss_resnet, test_acc_resnet = model_resnet.evaluate(test_resnet)\n",
        "print(f\"ResNet50   - Test Loss: {test_loss_resnet:.4f}, Test Accuracy: {test_acc_resnet:.4f}\")\n",
        "\n",
        "# Evaluate VGG16\n",
        "test_loss_vgg, test_acc_vgg = model_vgg.evaluate(test_vgg)\n",
        "print(f\"VGG16      - Test Loss: {test_loss_vgg:.4f}, Test Accuracy: {test_acc_vgg:.4f}\")\n",
        "\n",
        "# Evaluate MobileNetV2\n",
        "test_loss_mnet, test_acc_mnet = model_mnet.evaluate(test_mnet)\n",
        "print(f\"MobileNetV2 - Test Loss: {test_loss_mnet:.4f}, Test Accuracy: {test_acc_mnet:.4f}\")\n"
      ],
      "metadata": {
        "id": "zOw0blkzMAdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1.Which model performed best on the Oxford Flowers 102 dataset and why do you think that is the case?**\n",
        "ResNet50 performed best (~81.6% test accuracy), followed by MobileNetV2 (approx 77.5%) and VGG16 (approx 72.8%). Its deeper residual architecture and richer ImageNet features transfer better to fine‑grained flower details.\n",
        "\n",
        "**Q2.Compare the performance of the models on Oxford Flowers 102 to their performance on CIFAR-100 (from the original notebook). What differences do you observe and why?**\n",
        "All three models generally achieved higher accuracy on Oxford Flowers 102 than on CIFAR‑100, while keeping the same ranking (ResNet50 > MobileNetV2 > VGG16). Flowers are higher‑resolution, more similar to ImageNet content, and more visually distinctive than small 32×32 CIFAR‑100 objects, so transfer learning works better.\n",
        "\n",
        "**Q3.Discuss the effect of transfer learning on this dataset.**\n",
        "Transfer learning let us get strong results in a few epochs without huge data by starting from ImageNet weights instead of random initialization. Freezing most layers and only training a small head (plus light fine‑tuning) gave high accuracy and stable training compared to what training from scratch would require.\n",
        "\n",
        "**Q4.Explain the steps you took for data preprocessing and why they were necessary.**\n",
        "I resized all images to 224×224, applied model‑specific preprocess_input for ResNet50, VGG16, and MobileNetV2, converted labels to 102‑dim one‑hot vectors, and built shuffled, batched, prefetched tf.data pipelines. This matched the pre‑training setup and ensured efficient, correct input to each model.\n",
        "\n",
        "**Q5.Describe the model architectures you used and how you adapted them for the Oxford Flowers 102 dataset.**\n",
        "Each model used an ImageNet backbone (include_top=False) plus a new head: GlobalAveragePooling2D, dense layer(s) with ReLU and dropout, and a final Dense(102, softmax) classifier. You froze the base at first, then optionally unfroze top layers of ResNet50 with a lower learning rate to fine‑tune on flowers.\n",
        "\n",
        "**Q6.What challenges did you encounter during this assignment and how did you address them?**\n",
        "Main issues were long training time, potential overfitting, and keeping preprocessing consistent with each backbone. I simplified training loops, used EarlyStopping and ReduceLROnPlateau, and carefully applied the correct input size and preprocessing functions to stabilize and speed up training."
      ],
      "metadata": {
        "id": "QwSVr265_-us"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UwfsKoDnBPh3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}